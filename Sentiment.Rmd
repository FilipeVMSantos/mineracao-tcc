---
title: "Análise de Sentimento de Machado de Assis"
output: html_notebook
---

Carregando os pacotes

```{r}
library(pdftools)
library(tidyverse)
library(tidytext)
library(tm)
library(readtext)
```

Carregando os romances de Machado de Assis. As obras foram baixadas de no site do MEC [http://machado.mec.gov.br/obra-completa-lista/itemlist/category/23-romance]

```{r}
files_path = "/cloud/project/Obras"
my_corpus <- VCorpus(DirSource(files_path, pattern = ".pdf"), 
                     readerControl = list(reader = readPDF))
```

Convertendo em formato tidy

```{r}
mac_td = tidy(my_corpus, collapse = NULL) %>% 
  mutate(book = heading) %>% 
  unnest(text) %>% 
  group_by(heading) %>%
  mutate(page = 1:n()) %>% # Salva o número da página no arquivo pdf
  ungroup()
```

Removendo stopwords

```{r}
sw_tm = tibble(word = tm::stopwords(kind = "pt"))

tokens = mac_td %>% 
  unnest_tokens(word, text) %>% 
  anti_join(sw_tm, by = "word") %>% 
  select(c("word", "book", "page"))
```

Contagem de palavras

```{r}
mac_td %>% 
  unnest_tokens(word, text) %>% 
  count(word, sort = T) 
  
```


```{r}
tokens %>% count(word, sort = T)
```

Utilizando a lista de stopwords do nltk. (Arquivo disponibilizado em https://gist.github.com/alopes/5358189)
```{r}
sw_nltk = read_csv("nltk.txt", col_names = "word", col_types = "c")

tokens = mac_td %>% 
  unnest_tokens(word, text) %>% 
  anti_join(sw_nltk, by = "word") %>% 
  select(c("word", "book", "page"))

tokens %>% count(word, sort = T)
```

```{r}
sw_nltk[! sw_nltk$word %in% sw_tm$word, ]
```


```{r}
sw_nltk %>% distinct()
```


```{r}
sw_loh = read_csv("stopwords_loh.txt", col_names = "word", col_types = "c")

tokens = mac_td %>% 
  unnest_tokens(word, text) %>% 
  anti_join(sw_loh, by = "word") %>% 
  select(c("word", "book", "page"))

tokens %>% count(word, sort = T)
```

```{r}
sw_loh[! sw_loh$word %in% sw_nltk$word,] %>% filter(word =="é")
```

Combinando todas as listas de stop words

```{r}
sw = bind_rows(sw_tm, sw_loh, sw_nltk) %>% 
  distinct() %>% 
  arrange()

tokens = mac_td %>% 
  unnest_tokens(word, text) %>% 
  anti_join(sw, by = "word") %>% 
  select(c("word", "book", "page"))

tokens %>% count(word, sort = T)
```


Gráfico com as palavras mais comuns
```{r}
library(ggplot2)
library(ggpubr)

tokens %>%
  count(word, sort = TRUE) %>%
  filter(n > 700) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```


Carregando lexicon em português. Retirado do Kaggle [https://www.kaggle.com/rtatman/sentiment-lexicons-for-81-languages/data]

```{r}
poskaggle <- read.csv("positive_words_pt.txt", 
                      header = F, sep = "\t", strip.white = F, 
                      stringsAsFactors = F, encoding="UTF-8")
colnames(poskaggle) = "word"

negkaggle <- read.csv("negative_words_pt.txt", 
                      header = F, sep = "\t", strip.white = F, 
                      stringsAsFactors = F, encoding="UTF-8")
colnames(negkaggle) = "word"

poskaggle = poskaggle %>% mutate(sentiment = "positive")
negkaggle = negkaggle %>% mutate(sentiment = "negative")

sentiment = bind_rows(poskaggle, negkaggle)
```
Quantidade de palavras
```{r}
nrow(sentiment)
```
Proporção de positivas
```{r}
sum(sentiment$sentiment == "positive")/nrow(sentiment)
```

Contagem por sentimento

```{r}
sen_count = tokens %>% 
  inner_join(sentiment, by = "word") %>% 
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

sen_count
```
```{r}
words = mac_td %>% 
  unnest_tokens(word, text) %>% 
  select(c("word", "book", "page"))
```


```{r, fig.width=15, fig.height=15}
machado_sentiment = words %>% 
  inner_join(sentiment, by = "word") %>%
  group_by(book) %>% 
  count(book, index = page, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) 

ggplot(machado_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x") +
  theme(text = element_text(size=20)) +
  xlab("Página") +
  ylab("Sentimento")
```

Proporção de páginas positivas

```{r}
sum(machado_sentiment$sentiment > 0)/nrow(machado_sentiment)
```
 Saldo de Sentimento
 
```{r}
mean(machado_sentiment$sentiment)
```
 


Carregando lexicon
```{r}
library(lexiconPT)
```

Dicionário Léxico OpLexicon

```{r}
oplexicon <- read_csv('oplexicon_v3.0/lexico_v3.0.txt', 
                      col_names = c('word', 'type', 'polarity', 'other'), 
                      col_types = cols(
                          word = col_character(),
                          type = col_character(),
                          weight = col_integer(), 
                          other = col_character()
                        )
                      )

oplexicon = oplexicon_v3.0
colnames(oplexicon) = c('word', 'type', 'polarity', 'other')
```
Quantidade de palavras
```{r}
nrow(oplexicon)
```
Proporção de positivas
```{r}
sum(oplexicon$polarity == "1")/nrow(oplexicon)
```
Proporção Neutras

```{r}
sum(oplexicon$polarity == "0")/nrow(oplexicon)
```


```{r, fig.width=15, fig.height=15}
machado_op <- words %>% 
  inner_join(oplexicon, by = "word") %>% 
  group_by(book, page) %>%
  # group_by(index = page) %>%
  summarise(sentiment = sum(polarity)) #%>%
  # mutate(method = "AFINN")

ggplot(machado_op, aes(page, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x") +
  theme(text = element_text(size=20)) +
  xlab("Página") +
  ylab("Sentimento")
```

Proporção de páginas positivas

```{r}
sum(machado_op$sentiment > 0)/nrow(machado_op)
```
Saldo de Sentimento
 
```{r}
mean(machado_op$sentiment)
```

SentiLex
```{r, fig.width=15, fig.height=15}
# sentilex = sentiLex_lem_PT02
# colnames(sentilex) = c("word", "category", "polarity", "target", "classification")

sentilex = read_delim("SentiLex-PT02/SentiLex-flex-PT02.txt", ",", col_names = c("word", "data")) %>%
  mutate(polarity = str_extract(data, "POL:N.=.{1,2};")) %>%
  mutate(polarity = str_remove(polarity, "POL:N.=")) %>%
  mutate(polarity = as.numeric(str_remove(polarity, ";"))) %>%
  select(c(word, polarity)) 

machado_slex <- words %>% 
  inner_join(sentilex, by = "word") %>% 
  group_by(book, page) %>%
  # group_by(index = page) %>%
  summarise(sentiment = sum(polarity)) #%>%
  # mutate(method = "AFINN")

ggplot(machado_slex, aes(page, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x") +
  theme(text = element_text(size=20)) +
  xlab("Página") +
  ylab("Sentimento")
  
```

Quantidade de palavras
```{r}
nrow(sentilex)
```
Proporção positiva
```{r}
sum(sentilex$polarity >= 1)/nrow(sentilex)
```
Proporção Neutras
```{r}
sum(sentilex$polarity == 0)/nrow(sentilex)
```
Proporção de páginas positivas

```{r}
sum(machado_slex$sentiment > 0)/nrow(machado_slex)
```

Saldo de Sentimento
 
```{r}
mean(machado_slex$sentiment)
```
LIWC Portuguese Dictionary

```{r}
library(quanteda)
d = dictionary(file = "LIWC2007_Portugues_win.dic")
```
```{r, fig.width=15, fig.height=15}
liwc_positive = tibble(word = d$posemo, sentiment = "positive")
liwc_negative = tibble(word = d$negemo, sentiment = "negative")

liwc_sentiment = bind_rows(liwc_positive, liwc_negative)

machado_liwc = words %>% 
  inner_join(liwc_sentiment, by = "word") %>%
  group_by(book) %>% 
  count(book, index = page, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) 

ggplot(machado_liwc, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x") +
  theme(text = element_text(size=20))+
  xlab("Página") +
  ylab("Sentimento")
```
Quantidade de palavras
```{r}
nrow(liwc_sentiment)
```
Proporção de positivas
```{r}
sum(liwc_sentiment$sentiment == "positive")/nrow(liwc_sentiment)
```
Proporção de páginas positivas

```{r}
sum(machado_liwc$sentiment > 0)/nrow(machado_liwc)
```
 Saldo de Sentimento
 
```{r}
mean(machado_liwc$sentiment)
```

```{r}
words %>% 
  anti_join(sw, by = "word") %>%
  inner_join(sentiment, by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup() %>% 
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```
Wordcloud

```{r}
library(wordcloud2)

cloud = words %>%
  anti_join(sw) %>%
  count(word, sort = T) 

wordcloud2(cloud[1:200,],
           size = 0.5) 
```


N-grams

```{r}
bigrams = mac_td %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% sw$word) %>%
  filter(!word2 %in% sw$word) %>% 
  count(word1, word2, sort = TRUE)
```

Grafo

```{r, fig.width=10, fig.height=10}
library(igraph)

bigram_graph <- bigrams %>%
  filter(n > 20) %>%
  graph_from_data_frame()

library(ggraph)
set.seed(42)

# ggraph(bigram_graph, layout = "fr") +
#   geom_edge_link() +
#   geom_node_point() +
#   geom_node_text(aes(label = name), vjust = 1, hjust = 1)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 6) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```

Correlação
```{r, fig.width=10, fig.height=10}
library(widyr)

# section_words = mac_td %>%
#   # filter(book == "Pride & Prejudice") %>%
#   mutate(section = row_number() %/% 10) %>%
#   filter(section > 0) %>%
#   unnest_tokens(word, text) %>%
#   filter(!word %in% stop_words$word)

word_cors <- tokens %>%
  filter(!word %in% sw$word) %>% 
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, page, sort = TRUE) %>% 
  filter(correlation > .4)

word_cors %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

```

```{r, fig.width=10, fig.height=10}
library(widyr)

# section_words = mac_td %>%
#   # filter(book == "Pride & Prejudice") %>%
#   mutate(section = row_number() %/% 10) %>%
#   filter(section > 0) %>%
#   unnest_tokens(word, text) %>%
#   filter(!word %in% stop_words$word)

word_cors <- tokens %>%
  filter(!word %in% sw$word) %>% 
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, page, sort = TRUE) %>% 
  filter(correlation > .5)

word_cors %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

```
## Comparação entre dicionários

Formatando o OpLexicon e o SentiLex no mesmo formato dos outros para comparação

```{r}
sentilex_labels = sentilex %>% 
  mutate(sentiment = case_when(polarity == -1 ~ "negative",
                               polarity == 0 ~ "neutral",
                               polarity >= 1 ~ "positive")) %>% 
  select(c(word, sentiment))

oplexicon_labels = oplexicon %>% 
  mutate(sentiment = case_when(polarity == -1 ~ "negative",
                               polarity == 0 ~ "neutral",
                               polarity == 1 ~ "positive")) %>% 
  select(c(word, sentiment))
```

###Comparação Oplexicon x Sentilex

Palavras em comum

```{r}
(t_os = oplexicon_labels %>% bind_rows(sentilex_labels) %>% distinct(word) %>% nrow())

```

```{r}
opl_stl = inner_join(oplexicon_labels, sentilex_labels, by = "word") %>% 
  mutate(agree = (sentiment.x == sentiment.y))
```

Palavras em comum
```{r}
(c_os = nrow(opl_stl))
```
Porcentagem palavras em comum
```{r}
c_os/t_os
```

```{r}
sum(opl_stl$agree)/nrow(opl_stl)
```


###Comparação Multilingual x Oplexicon
Palavras em comum
```{r}
(t_mo = oplexicon_labels %>% bind_rows(sentiment) %>% distinct(word) %>% nrow())
```

```{r}
mult_opl = inner_join(oplexicon_labels, sentiment, by = c("word" = "word")) %>% 
  mutate(agree = (sentiment.x == sentiment.y))
```

```{r}
(c_mo = nrow(mult_opl))
```
```{r}
c_mo/t_mo
```

```{r}
sum(mult_opl$agree)/nrow(mult_opl)
```

Comparação Oplexicon x LIWC

```{r}
(t_ol = oplexicon_labels %>% bind_rows(liwc_sentiment) %>% distinct(word) %>% nrow())
```

```{r}
opl_liwc = inner_join(oplexicon_labels, liwc_sentiment, by = c("word" = "word")) %>% 
  mutate(agree = (sentiment.x == sentiment.y))
```
```{r}
(c_ol = nrow(opl_liwc))
```

```{r}
sum(opl_liwc$agree)/nrow(opl_liwc)
```
```{r}
c_ol/t_ol
```

###Comparação Sentilex x Multilingual

```{r}
(t_sm = sentilex_labels %>% bind_rows(sentiment) %>% distinct(word) %>% nrow())
```


```{r}
mult_stl = inner_join(sentilex_labels, sentiment, by = c("word" = "word")) %>% 
  mutate(agree = (sentiment.x == sentiment.y))
```
```{r}
(c_sm = nrow(mult_stl))
```
```{r}
c_sm/t_sm
```


```{r}
sum(mult_stl$agree)/nrow(mult_stl)
```

###Comparação Sentilex x LIWC
```{r}
(t_sl = sentilex_labels %>% bind_rows(liwc_sentiment) %>% distinct(word) %>% nrow())
```


```{r}
stl_liwc = inner_join(sentilex_labels, liwc_sentiment, by = c("word" = "word")) %>% 
  mutate(agree = (sentiment.x == sentiment.y))
```
```{r}
(c_sl = nrow(stl_liwc))
```
```{r}
c_sl/t_sl
```

```{r}
sum(stl_liwc$agree)/nrow(stl_liwc)
```

###Comparação LIWC x Multilingual
```{r}
(t_lm = liwc_sentiment %>% bind_rows(sentiment) %>% distinct(word) %>% nrow())
```


```{r}
mult_liwc = inner_join(liwc_sentiment, sentiment, by = c("word" = "word")) %>% 
  mutate(agree = (sentiment.x == sentiment.y))
```
```{r}
(c_lm = nrow(mult_liwc))
```

```{r}
c_lm/t_lm
```

```{r}
sum(mult_liwc$agree)/nrow(mult_liwc)
```


